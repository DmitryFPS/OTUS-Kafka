0) Запускаем демонстрацию

0.1) Запускаем сервисы
docker compose up -d

0.2) Проверяем контейнеры
docker compose ps -a

0.3) Проверям логи Kafka Connect
docker logs -f connect
^C


1) Основы ksqlDB

1.1) Проверяем список топиков
docker exec kafka1 kafka-topics --list --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092

1.2) Запускаем ksqlDB CLI
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088

1.3) Устанавливаем чтение с начала темы
SET 'auto.offset.reset' = 'earliest';

1.4) Проверяем топики
SHOW TOPICS;

1.5) Создаём поток, который читает топик users
CREATE STREAM users (ROWKEY INT KEY, USERNAME VARCHAR) WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON', PARTITIONS=3);

1.6) Работа с потоками
SHOW STREAMS;
SHOW STREAMS EXTENDED;
DESCRIBE users;
DESCRIBE users EXTENDED;

1.7) Добавим записи в поток
INSERT INTO users (username) VALUES ('Alex');
INSERT INTO users (username) VALUES ('Barbara');
INSERT INTO users (username) VALUES ('Carl');
INSERT INTO users (username) VALUES ('Fiona');

1.8) Прочитаем записи (pull-запрос) в потоке
SELECT * FROM users;

1.9) Прочитаем записи в топике
PRINT users FROM BEGINNING;

1.10) Создадим push-запрос
SELECT 'Hello, ' + USERNAME AS GREETING FROM users EMIT CHANGES;

1.11) Открываем второй терминал, добавляем записи в поток
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088
INSERT INTO users (username) VALUES ('Mary');
INSERT INTO users (username) VALUES ('Garry');
INSERT INTO users (username) VALUES ('Ann');

1.12) В первом терминале посмотрим на метаданные
^C
SELECT ROWTIME, ROWOFFSET, ROWPARTITION, * FROM users EMIT CHANGES;

1.13) Во втором терминале работаем с запросами
LIST QUERIES;
SHOW QUERIES EXTENDED;

1.14) Во втором терминале работаем с функциями
SHOW FUNCTIONS;
DESCRIBE FUNCTION UUID;

1.15) Во первом терминале завершаем push-запрос
^C

## Непрерывные запросы

1.16) В первом терминале создаём первый поток
CREATE STREAM PurchaseStream (id INT KEY, product VARCHAR, left_ts VARCHAR) WITH (KAFKA_TOPIC='PurchaseTopic', VALUE_FORMAT='JSON', TIMESTAMP='left_ts', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssX', PARTITIONS=3);

1.17) Создаём второй поток
CREATE STREAM PaymentStream (ID INT KEY, purchaseId INT, status VARCHAR, right_ts VARCHAR) WITH (KAFKA_TOPIC='PaymentTopic', VALUE_FORMAT='JSON', TIMESTAMP='right_ts', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssX', PARTITIONS=3);

1.18) Создаём CSAS - Create Stream As Select
CREATE STREAM PaymentPurchaseStream WITH (KAFKA_TOPIC = 'PaymentPurchaseTopic', VALUE_FORMAT='JSON')
AS SELECT l.id AS purchaseId, l.product, r.status
FROM PurchaseStream l
INNER JOIN PaymentStream r WITHIN 7 DAYS ON l.id = r.purchaseId
EMIT CHANGES;

1.19) Добавим записи в потоки
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (1,  'kettle', '2022-01-29T06:01:18Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (2,  'grill' , '2022-01-29T17:02:20Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (3,  'toaster', '2022-01-29T13:44:10Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (4,  'hair dryer', '2022-01-29T11:58:25Z');

INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (101, 1, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (103, 3, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (104, 4, 'OK', '2022-01-29T12:08:25Z');

1.20) Проверяем объединённый поток
SELECT * FROM PaymentPurchaseStream;

## Агрегирование

1.21) Во втором терминале посчитаем количество записей по каждому id
SELECT id, count(*) as count FROM PaymentStream GROUP BY id EMIT CHANGES;

1.22) В первом терминале добавим записи в поток
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (101, 2, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (101, 3, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (103, 2, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (104, 4, 'OK', '2022-01-29T12:08:25Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (104, 1, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (104, 2, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (103, 2, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (101, 3, 'OK', '2022-01-29T13:54:10Z');


2) ksqlDB + Kafka Connect, Вариант 1

2.1) Проверяем статус и плагины коннекторов
curl http://localhost:8083 | jq
curl http://localhost:8083/connector-plugins | jq

2.2) Проверяем список топиков
docker exec kafka1 kafka-topics --list --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092

2.3) Создадим таблицу в PostgreSQL и запишем в неё данные
docker exec -ti postgres psql -U postgres
CREATE TABLE titles (id SERIAL PRIMARY KEY, title VARCHAR(120));
INSERT INTO titles (title) values ('Stranger Things');
INSERT INTO titles (title) values ('Black Mirror');
INSERT INTO titles (title) values ('The Office');
SELECT * FROM titles;
\q

2.4) Запускаем ksqlDB CLI
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088

2.5) Устанавливаем чтение с начала темы
SET 'auto.offset.reset' = 'earliest';

2.6) Создаём коннектор postgres-source
CREATE SOURCE CONNECTOR `postgres-source` WITH (
    "connector.class" = 'io.confluent.connect.jdbc.JdbcSourceConnector',
    "connection.url" = 'jdbc:postgresql://postgres:5432/postgres?user=postgres&password=password',
    "mode" = 'incrementing',
    "incrementing.column.name" = 'id',
    "table.whitelist" = 'titles',
    "topic.prefix" = 'postgres.',
    "key" = 'id');

2.7) Выводим список коннекторов
SHOW CONNECTORS;

2.8) Получаем описание коннектора inventory-connector
DESCRIBE CONNECTOR `postgres-source`;

2.9) Проверяем топики
SHOW TOPICS;

2.10) Выведем содержимое топика
PRINT `postgres.titles` FROM BEGINNING;
^C

2.11) Создаём поток, который читает топик postgres.titles
CREATE STREAM titles WITH (kafka_topic = 'postgres.titles', value_format = 'avro');

2.12) Выведем описание потока
DESCRIBE titles EXTENDED;

2.13) Прочитаем записи (pull-запрос)
SELECT * FROM titles;


3) ksqlDB + Kafka Connect, Вариант 2

3.1) Создадим таблицу в PostgreSQL и запишем в неё данные
docker exec -ti postgres psql -U postgres
CREATE TABLE customers (id TEXT PRIMARY KEY, name TEXT, age INT);
INSERT INTO customers (id, name, age) VALUES ('5', 'Fred', 34);
INSERT INTO customers (id, name, age) VALUES ('7', 'Sue', 25);
INSERT INTO customers (id, name, age) VALUES ('2', 'Bill', 51);
SELECT * FROM customers;
\q

3.2) Запускаем ksqlDB CLI
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088

3.3) Устанавливаем чтение с начала темы
SET 'auto.offset.reset' = 'earliest';

3.4) Создаём коннектор customers_source
CREATE SOURCE CONNECTOR `customers_source` WITH (
    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',
    'database.hostname' = 'postgres',
    'database.port' = '5432',
    'database.user' = 'postgres',
    'database.password' = 'password',
    'database.dbname' = 'postgres',
    'database.server.name' = 'postgres',
    'table.include.list' = 'public.customers',
    'topic.prefix' = 'postgres',
    'key.converter' = 'org.apache.kafka.connect.json.JsonConverter',
    'key.converter.schemas.enable' = 'false',
    'transforms' = 'unwrap',
    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',
    'transforms.unwrap.drop.tombstones' = 'true',
    'transforms.unwrap.delete.handling.mode' = 'rewrite',
    'plugin.name' = 'pgoutput',
    'tasks.max' = '1'
);

3.5) Выводим список коннекторов
SHOW CONNECTORS;

3.6) Получаем описание коннектора inventory-connector
DESCRIBE CONNECTOR `customers_source`;

3.7) Проверяем топики
SHOW TOPICS;

3.8) Выведем содержимое топика
PRINT `postgres.public.customers` FROM BEGINNING;
^C

3.9) Создаём поток
CREATE STREAM customers WITH (kafka_topic = 'postgres.public.customers', value_format = 'avro');

3.10) Выведем содержимое потока
SELECT * FROM customers EMIT CHANGES;

3.11) Во втором терминале обновляем записи в таблице
docker exec -ti postgres psql -U postgres
INSERT INTO customers (id, name, age) VALUES ('3', 'Ann', 18);
UPDATE customers set age = 35 WHERE id = '5';
DELETE FROM customers WHERE id = '2';
\q

3.12) В первом терминале создаём таблицу
^C
CREATE TABLE customers_by_key AS
    SELECT id,
           latest_by_offset(name, false) AS name,
           latest_by_offset(age, false) AS age
    FROM customers
    GROUP BY id
    EMIT CHANGES;

3.13) Создадим запрос к таблице
SELECT * FROM customers_by_key;


4) Завершаем работу
docker compose down
docker container prune -f
docker volume prune -f
docker network prune -f

